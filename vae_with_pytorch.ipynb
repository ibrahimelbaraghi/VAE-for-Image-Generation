{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"9DpHnufN9rEu"},"outputs":[],"source":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aClXlmQBesHW","executionInfo":{"status":"ok","timestamp":1703968555229,"user_tz":-60,"elapsed":14304,"user":{"displayName":"Ibrahim El baraghi","userId":"07387594064977257772"}},"outputId":"aedd64a9-b159-4f4b-b6a8-f2704533e3d1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hNBeuD7n90Sj"},"outputs":[],"source":["import imageio\n","import numpy as np\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt\n","from torchvision.utils import save_image\n","to_pil_image = transforms.ToPILImage()\n","\n","def save_reconstructed_images(recon_images, epoch):\n","    save_image(recon_images.cpu(), f\"./outputs/output{epoch}.jpg\")\n","def save_loss_plot(train_loss, valid_loss):\n","    # loss plots\n","    plt.figure(figsize=(10, 7))\n","    plt.plot(train_loss, color='orange', label='train loss')\n","    plt.plot(valid_loss, color='red', label='validataion loss')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","    plt.savefig('./outputs/loss.jpg')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ljls2UbN91RJ"},"outputs":[],"source":["from tqdm import tqdm\n","import torch\n","def final_loss(bce_loss, mu, logvar):\n","    BCE = bce_loss\n","    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    return BCE + KLD"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pTqmcETY9-Dr"},"outputs":[],"source":["def train(model, dataloader, dataset, device, optimizer, criterion):\n","    model.train()\n","    running_loss = 0.0\n","    counter = 0\n","    for i, batch_data in tqdm(enumerate(dataloader), total=int(len(dataset)/dataloader.batch_size)):\n","        counter += 1\n","        batch_data = batch_data[0]\n","        batch_data = batch_data.to(device)\n","        #print(batch_data.size())\n","        optimizer.zero_grad()\n","        reconstruction, mu, logvar = model(batch_data)\n","        bce_loss = criterion(reconstruction, batch_data)\n","        loss = final_loss(bce_loss, mu, logvar)\n","        loss.backward()\n","        running_loss += loss.item()\n","        optimizer.step()\n","    train_loss = running_loss / counter\n","    return train_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TTERFg4X-Byf"},"outputs":[],"source":["def validate(model, dataloader, dataset, device, criterion):\n","    model.eval()\n","    running_loss = 0.0\n","    counter = 0\n","    with torch.no_grad():\n","        for i, batch_data in tqdm(enumerate(dataloader), total=int(len(dataset)/dataloader.batch_size)):\n","            counter += 1\n","            batch_data = batch_data[0]\n","            batch_data = batch_data.to(device)\n","            reconstruction, mu, logvar = model(batch_data)\n","            bce_loss = criterion(reconstruction, batch_data)\n","            loss = final_loss(bce_loss, mu, logvar)\n","            running_loss += loss.item()\n","\n","            # save the last batch input and output of every epoch\n","            if i == int(len(dataset)/dataloader.batch_size) - 1:\n","                recon_images = reconstruction\n","    val_loss = running_loss / counter\n","    return val_loss, recon_images"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WHuF6O8B-F5q"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","kernel_size = 4 # (4, 4) kernel\n","init_channels = 8 # initial number of filters\n","image_channels = 1 # MNIST images are grayscale\n","latent_dim = 16 # latent dimension for sampling"]},{"cell_type":"markdown","source":["# Classe de l'Encodeur"],"metadata":{"id":"BO9i7O4sDEGa"}},{"cell_type":"code","source":["class VAE_Encoder(nn.Module):\n","  def __init_(self, image_channels, init_channels, kernel_size, latent_dim):\n","    super(VAE_Encoder, self).__init__()\n","\n","    self.conv1 = nn.Conv2d(\n","      in_channels=image_channels, out_channels=init_channels, kernel_size=kernel_size,\n","      stride=2, padding=1\n","    )\n","    self.conv2 = nn.Conv2d(\n","      in_channels=init_channels, out_channels=init_channels * 2, kernel_size=kernel_size,\n","      stride=2, padding=1\n","    )\n","    self.conv3 = nn.Conv2d(\n","      in_channels=init_channels * 2, out_channels=init_channels * 4, kernel_size=kernel_size,\n","      stride=2, padding=1\n","    )\n","    self.conv4 = nn.Conv2d(\n","      in_channels=init_channels * 4, out_channels=64, kernel_size=kernel_size,\n","      stride=2, padding=0\n","    )\n","\n","    self.fc1 = nn.Linear(64, 128)\n","    self.fc_mu = nn.Linear(128, latent_dim)\n","    self.fc_log_var = nn.Linear(128, latent_dim)\n","\n","    def forward(self, x):\n","      x = F.relu(self.conv1(x))\n","      x = F.relu(self.conv2(x))\n","      x = F.relu(self.conv3(x))\n","      x = F.relu(self.conv4(x))\n","      batch, _, _, _ = x.shape\n","      x = F.adaptive_avg_pool2d(x, 1).reshape(batch, -1)\n","      hidden = F.relu(self.fc1(x))\n","      mu = self.fc_mu(hidden)\n","      log_var = self.fc_log_var(hidden)\n","      return mu, log_var"],"metadata":{"id":"MtyCWJsF-uRN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Classe du décodeur"],"metadata":{"id":"xJzArBGcC5tS"}},{"cell_type":"code","source":["class VAE_Decoder(nn.Module):\n","  def __init__(self, image_channels, init_channels, kernel_size, latent_dim):\n","    super(VAE_Decoder, self).__init__()\n","\n","    self.fc2 = nn.Linear(latent_dim, 64)\n","    self.deconv1 = nn.ConvTranspose2d(\n","      in_channels=64, out_channels=init_channels * 8, kernel_size=kernel_size,\n","      stride=1, padding=0\n","    )\n","    self.deconv2 = nn.ConvTranspose2d(\n","      in_channels=init_channels * 8, out_channels=init_channels * 4, kernel_size=kernel_size,\n","      stride=2, padding=1\n","    )\n","    self.deconv3 = nn.ConvTranspose2d(\n","      in_channels=init_channels * 4, out_channels=init_channels * 2, kernel_size=kernel_size,\n","      stride=2, padding=1\n","    )\n","    self.deconv4 = nn.ConvTranspose2d(\n","      in_channels=init_channels * 2, out_channels=image_channels, kernel_size=kernel_size,\n","      stride=2, padding=1\n","    )\n","\n","    def forward(self, z):\n","      z = F.relu(self.fc2(z))\n","      z = z.view(-1, 64, 1, 1)\n","      x = F.relu(self.deconv1(z))\n","      x = F.relu(self.deconv2(x))\n","      x = F.relu(self.deconv3(x))\n","      reconstruction = torch.sigmoid(self.deconv4(x))\n","      return reconstruction"],"metadata":{"id":"KkUnevj--t7d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Classe du modèle (Encodeur + Décodeur)"],"metadata":{"id":"lWZahDImDX9G"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"t8gCIO2g-J2S"},"outputs":[],"source":["# define a Conv VAE\n","class ConvVAE(nn.Module):\n","    def __init__(self, encoder, decoder):\n","      super(ConvVAE, self).__init__()\n","      self.encoder = encoder;\n","      self.decoder = decoder;\n","\n","    def sample(self, x):\n","      mu, log_var = self.encoder()\n","      std = torch.exp(0.5*log_var) # standard deviation\n","      eps = torch.randn_like(std) # `randn_like` as we need the same size\n","      sample = mu + (eps * std) # sampling\n","      return sample\n","\n","    def forward(self, x):\n","      # encoding\n","      x = F.relu(self.enc1(x))\n","      x = F.relu(self.enc2(x))\n","      x = F.relu(self.enc3(x))\n","      x = F.relu(self.enc4(x))\n","      batch, _, _, _ = x.shape\n","      x = F.adaptive_avg_pool2d(x, 1).reshape(batch, -1)\n","      hidden = self.fc1(x)\n","      # get `mu` and `log_var`\n","      mu = self.fc_mu(hidden)\n","      log_var = self.fc_log_var(hidden)\n","      # get the latent vector through reparameterization\n","      z = self.sample(mu, log_var)\n","      z = self.fc2(z)\n","      z = z.view(-1, 64, 1, 1)\n","\n","      # decoding\n","      x = F.relu(self.dec1(z))\n","      x = F.relu(self.dec2(x))\n","      x = F.relu(self.dec3(x))\n","      reconstruction = torch.sigmoid(self.dec4(x))\n","      return reconstruction, mu, log_var"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PgiXrpjFBMjM"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xmT3TCKx-WWu"},"outputs":[],"source":["import os\n","import torch\n","import torch.optim as optim\n","import torch.nn as nn\n","#import model\n","import torchvision.transforms as transforms\n","import torchvision\n","import matplotlib\n","from torch.utils.data import DataLoader\n","from torchvision.utils import make_grid\n","matplotlib.style.use('ggplot')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b9v_5LrK-i9n"},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","# initialize the model\n","model = ConvVAE().to(device)\n","# set the learning parameters\n","lr = 0.001\n","epochs = 100\n","batch_size = 64\n","optimizer = optim.Adam(model.parameters(), lr=lr)\n","criterion = nn.BCELoss(reduction='sum')\n","# a list to save all the reconstructed images in PyTorch grid format\n","grid_images = []"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1703957667147,"user":{"displayName":"Ibrahim El baraghi","userId":"07387594064977257772"},"user_tz":-60},"id":"d1uAiIrVBpTJ","outputId":"d7064a73-5662-4f52-e537-1a2d2d12ec5a"},"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1            [-1, 8, 16, 16]             136\n","            Conv2d-2             [-1, 16, 8, 8]           2,064\n","            Conv2d-3             [-1, 32, 4, 4]           8,224\n","            Conv2d-4             [-1, 64, 1, 1]          32,832\n","            Linear-5                  [-1, 128]           8,320\n","            Linear-6                   [-1, 16]           2,064\n","            Linear-7                   [-1, 16]           2,064\n","            Linear-8                   [-1, 64]           1,088\n","   ConvTranspose2d-9             [-1, 64, 4, 4]          65,600\n","  ConvTranspose2d-10             [-1, 32, 8, 8]          32,800\n","  ConvTranspose2d-11           [-1, 16, 16, 16]           8,208\n","  ConvTranspose2d-12            [-1, 1, 32, 32]             257\n","================================================================\n","Total params: 163,657\n","Trainable params: 163,657\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.09\n","Params size (MB): 0.62\n","Estimated Total Size (MB): 0.72\n","----------------------------------------------------------------\n"]}],"source":["import torch\n","import torch.nn as nn\n","from torchsummary import summary\n","\n","summary(model=model, input_size=(1, 32, 32))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xp-BqerF-23H"},"outputs":[],"source":["transform = transforms.Compose([\n","    transforms.Resize((32, 32)),\n","    transforms.ToTensor(),\n","])\n","# training set and train data loader\n","trainset = torchvision.datasets.MNIST(\n","    root='../input', train=True, download=True, transform=transform\n",")\n","trainloader = DataLoader(\n","    trainset, batch_size=batch_size, shuffle=True\n",")\n","# validation set and validation data loader\n","testset = torchvision.datasets.MNIST(\n","    root='../input', train=False, download=True, transform=transform\n",")\n","testloader = DataLoader(\n","    testset, batch_size=batch_size, shuffle=False\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QUbkh_WD-5Gn"},"outputs":[],"source":["train_loss = []\n","valid_loss = []\n","for epoch in range(epochs):\n","    print(f\"Epoch {epoch+1} of {epochs}\")\n","    train_epoch_loss = train(\n","        model, trainloader, trainset, device, optimizer, criterion\n","    )\n","    valid_epoch_loss, recon_images = validate(\n","        model, testloader, testset, device, criterion\n","    )\n","    train_loss.append(train_epoch_loss)\n","    valid_loss.append(valid_epoch_loss)\n","    # save the reconstructed images from the validation loop\n","    #save_reconstructed_images(recon_images, epoch+1)\n","    # convert the reconstructed images to PyTorch image grid format\n","    image_grid = make_grid(recon_images.detach().cpu())\n","    grid_images.append(image_grid)\n","    print(f\"Train Loss: {train_epoch_loss:.4f}\")\n","    print(f\"Val Loss: {valid_epoch_loss:.4f}\")\n","\n","# enregistrer le modèle\n","checkpoint_dir = '/content/drive/MyDrive/VAE/model'\n","os.makedirs(checkpoint_dir, exist_ok=True)\n","model_save_path = os.path.join(checkpoint_dir, 'final_model.pth')\n","torch.save(model.state_dict(), model_save_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VUkQ0P5i_K__","executionInfo":{"status":"ok","timestamp":1703957684275,"user_tz":-60,"elapsed":2239,"user":{"displayName":"Ibrahim El baraghi","userId":"07387594064977257772"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"7f3402ab-7009-4ff1-a5f2-9029277b8bac"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":25}],"source":["file_path = '/content/drive/MyDrive/VAE/model/final_model.pth'\n","checkpoint = torch.load(file_path)\n","model.load_state_dict(checkpoint)"]},{"cell_type":"code","source":["def generate_new_image():\n","  # Assuming z_mean and z_log_var are tensors\n","  z_sample = model.sample(self, mu, log_var)\n","\n","  # Pass through the decoder\n","  z = self.fc2(z_sample)\n","  z = z.view(-1, 64, 1, 1)\n","  x = F.relu(self.dec1(z))\n","  x = F.relu(self.dec2(x))\n","  x = F.relu(self.dec3(x))\n","  generated_image = torch.sigmoid(self.dec4(x))\n","  return generated_image"],"metadata":{"id":"ngqjnD3c9qry"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.generate_new_image()"],"metadata":{"id":"Cu7wvrciiO1s"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"mount_file_id":"18zhAj50LhmYDE5VDdwDrwU5Vlrdf4Ano","authorship_tag":"ABX9TyNl5aGk4YZIzcS+5fEfbu4V"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}